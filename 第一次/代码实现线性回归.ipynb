{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,80) and (8000,1) not aligned: 80 (dim 1) != 8000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-78252b701bbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# 最小二乘法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mmodel_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'least_squares'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mmodel_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[0my_pred_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Least Squares Coefficients:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-78252b701bbf>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mXTX\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1e-8\u001b[0m  \u001b[1;31m# 正则化以避免奇异矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"矩阵不可逆，无法使用最小二乘法求解。\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,80) and (8000,1) not aligned: 80 (dim 1) != 8000 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    # 实现线性回归算法\n",
    "    # 指定方法，梯度下降法的学习率，最大迭代次数，收敛阈值\n",
    "    def __init__(self, method='gradient_descent', learning_rate=0.01, max_iters=1000, tol=1e-4):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.theta = None  # 存储类型的参数\n",
    "        self.mean = None  # 存储数据的均值\n",
    "        self.std = None  # 存储标准差\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"\n",
    "        为特征矩阵 X 添加截距项\n",
    "        :param X: 特征矩阵\n",
    "        :return: 带有截距项的特征矩阵\n",
    "        \"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "\n",
    "    def _normalize(self, X):\n",
    "        \"\"\"\n",
    "        对特征矩阵 X 进行标准化处理\n",
    "        :param X: 特征矩阵\n",
    "        :return: 标准化后的特征矩阵\n",
    "        \"\"\"\n",
    "        if self.mean is None or self.std is None:\n",
    "            self.mean = np.mean(X, axis=0)   #计算每列的均值\n",
    "            self.std = np.std(X, axis=0)     #计算每列的标准差\n",
    "            self.std[self.std == 0] = 1  # 把标准差为0的替换为1，避免除以零\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        训练线性回归模型\n",
    "        :param X: 特征矩阵\n",
    "        :param y: 目标向量\n",
    "        :return: 训练好的模型\n",
    "        \"\"\"\n",
    "        X = self._add_intercept(X)  #添加截距项\n",
    "        y = y.reshape(-1, 1)   #目标销量y转化为列向量\n",
    "        m, n = X.shape #获取X的行数和列数\n",
    "\n",
    "        if self.method == 'least_squares':  #使用最小二乘法\n",
    "            XTX = X.T.dot(X)\n",
    "            XTX += np.eye(n) * 1e-8  # 正则化以避免奇异矩阵\n",
    "            try:\n",
    "                self.theta = np.linalg.inv(XTX).dot(X.T).dot(y)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"矩阵不可逆，无法使用最小二乘法求解。\")\n",
    "                return self\n",
    "        elif self.method == 'gradient_descent':   #使用梯度下降法\n",
    "            X_features = X[:, 1:]  #获取X除去截距项的部分\n",
    "            X_features = self._normalize(X_features)  #标准化分析\n",
    "            X = np.hstack((X[:, 0].reshape(-1, 1), X_features))  #标准化后的特征矩阵与截距项拼接\n",
    "            self.theta = np.zeros((n, 1))  #初始化参数向量为零向量\n",
    "            prev_loss = float('inf')\n",
    "            for _ in range(self.max_iters):  #进行最大迭代次数的迭代\n",
    "                y_pred = X.dot(self.theta)  #计算预测值\n",
    "                error = y_pred - y  #计算误差值\n",
    "                loss = np.sum(error ** 2) / (2 * m)  #计算损失函数\n",
    "                gradient = X.T.dot(error) / m  #计算梯度\n",
    "                self.theta -= self.learning_rate * gradient  #更新参数向量\n",
    "                if abs(prev_loss - loss) < self.tol:      \n",
    "                    break\n",
    "                prev_loss = loss\n",
    "        else:\n",
    "            raise ValueError(\"Method not supported. Use 'least_squares' or 'gradient_descent'\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        使用训练好的模型进行预测\n",
    "        :param X: 特征矩阵\n",
    "        :return: 预测结果\n",
    "        \"\"\"\n",
    "        X = self._add_intercept(X)\n",
    "        if self.method == 'gradient_descent':\n",
    "            X_features = X[:, 1:]\n",
    "            X_features = (X_features - self.mean) / self.std  #对特征矩阵进行标准化处理\n",
    "            X = np.hstack((X[:, 0].reshape(-1, 1), X_features)) #拼接\n",
    "        return X.dot(self.theta).flatten()  #计算预测值并将其转化为一维数组\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        计算均方误差\n",
    "        :param y_true: 真实值\n",
    "        :param y_pred: 预测值\n",
    "        :return: 均方误差\n",
    "        \"\"\"\n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(\"y_true 和 y_pred 的长度必须一致。\")\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def r2_score(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        计算 R² 分数\n",
    "        :param y_true: 真实值\n",
    "        :param y_pred: 预测值\n",
    "        :return: R² 分数\n",
    "        \"\"\"\n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(\"y_true 和 y_pred 的长度必须一致。\")\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)  #计算残差平方和\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  #计算总平方和\n",
    "        return 1 - (ss_res / ss_tot) \n",
    "\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成示例数据\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1).ravel()\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    split = 80\n",
    "    X_train, y_train = X[:split], y[:split]  #划分训练集\n",
    "    X_test, y_test = X[split:], y[split:]  #划分测试集\n",
    "\n",
    "    # 最小二乘法\n",
    "    model_ls = LinearRegression(method='least_squares')\n",
    "    model_ls.fit(X_train, y_train)  #训练模型\n",
    "    y_pred_ls = model_ls.predict(X_test) #进行预测\n",
    "    print(\"Least Squares Coefficients:\", model_ls.theta.T)\n",
    "    print(\"MSE (Least Squares):\", model_ls.mse(y_test, y_pred_ls))\n",
    "    print(\"R² (Least Squares):\", model_ls.r2_score(y_test, y_pred_ls))\n",
    "\n",
    "    # 梯度下降法\n",
    "    model_gd = LinearRegression(method='gradient_descent', learning_rate=0.1, max_iters=1000)\n",
    "    model_gd.fit(X_train, y_train)\n",
    "    y_pred_gd = model_gd.predict(X_test)\n",
    "    print(\"\\nGradient Descent Coefficients:\", model_gd.theta.T)\n",
    "    print(\"MSE (Gradient Descent):\", model_gd.mse(y_test, y_pred_gd))\n",
    "    print(\"R² (Gradient Descent):\", model_gd.r2_score(y_test, y_pred_gd))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
